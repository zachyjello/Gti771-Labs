{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjgFUch8bLwE"
      },
      "source": [
        "# GTI771 - Apprentissage machine avancé\n",
        "## Département de génie logiciel et des technologies de l’information (LogTI)\n",
        "\n",
        "## Laboratoire 1 - Préparation des données\n",
        "#### <font color=black> Version 1 - Janvier 2025 - Chargé de lab. Arthur Josi </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G_KivoknmxT"
      },
      "source": [
        "Les laboratoires sont à faire par groupe de deux ou trois étudiants. Favorisez les groupes de trois."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSx7LnYLbLwG"
      },
      "source": [
        "| NOMS                  | CODE PERMANENT                                   |\n",
        "|-----------------------|--------------------------------------------------|\n",
        "| Étudiant1             | Code1                                            |\n",
        "| Zacharie Morin        | MORZ63310201                                     |\n",
        "| Raphael Roumat        | ROUR79040002                                     |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuR8ueyebLwG"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Ce premier laboratoire porte sur la préparation de données pour l'apprentissage machine, et l'application d'approche simple de classification. Le problème qui vous est présenté est le problème de fraude bancaire [Bank Account Fraud (NeurIPS 2022)](https://www.kaggle.com/datasets/sgpjesus/bank-account-fraud-dataset-neurips-2022), dont le but est de detecter les fraudes !\n",
        "\n",
        "Le dataset a été légèrement modifier pour atteindre les objectifs des différents laboratoire et rendre l'application d'algorithme plus rapide.\n",
        "\n",
        "Veuillez noter que les données qui vous sont fournies ne sont pas directement adaptées à leur analyse. Une approche naïve où l'on appliquerait un algorithme d'apprentissage machine sans analyse des données au préalable est toujours à éviter, et ceci inclus les laboratoires de ce cours. Au cours de ce premier laboratoire, nous allons donc explorer les données, les préparer, et appliquer une méthode de \"template matching\" pour déterminer si une transaction est une fraude ou non.\n",
        "\n",
        "\n",
        "Au cours de chacun des laboratoires, l'évaluation sera basée sur:\n",
        "- les réponses aux questions du notebook;\n",
        "- la justification et pertinence des algorithmes proposés et utilisés ;\n",
        "- l'organisation de votre code source (SVP, pensez à commenter votre code source);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "nb5c_PGPbLwG"
      },
      "source": [
        "# Modules et bibliotèques python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPZudkRhbLwG"
      },
      "source": [
        "### Import de bibliotèques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNG3ZDNYbLwG"
      },
      "source": [
        "Ajouter une courte description aux bibliothèques que vous allez utiliser pour compléter ce notebook. N'hésitez pas à importer celles qui vous sont utiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-11T16:48:15.969300Z",
          "start_time": "2025-05-11T16:48:15.951796Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqWHnNHmbLwH",
        "outputId": "ad028b35-e823-45a9-8e61-5b15dff75c19"
      },
      "outputs": [],
      "source": [
        "%pip install pandas seaborn matplotlib scikit-learn\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rsy-BvW3bLwH"
      },
      "source": [
        "# Partie 1 - Analyse exploratoire des données\n",
        "\n",
        "On va commencer par regarder les données, c'est une pratique indispensable.\n",
        "\n",
        "Pour ce lab, nous allons utiliser le dataset Bank Account Fraud (BAF). **La version à utiliser est celle du moodle.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6gST-LmCT6w"
      },
      "source": [
        "###  <font color=blue> Questions: </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgFbpsY8BBT0"
      },
      "source": [
        "1. Loader le dataset depuis le fichier csv (en utilisant la librairie [pandas](https://pandas.pydata.org/docs/getting_started/index.html#getting-started) par exemple) et afficher le tableau de données. Si vous mettez vos données dans google colab, vous pouvez normalement y accéder au path: `/content/BAF_200k.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-12T00:16:09.985187Z",
          "start_time": "2025-05-12T00:16:06.713487Z"
        },
        "id": "UdCmTZ-hbLwI"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fraud_bool</th>\n",
              "      <th>income</th>\n",
              "      <th>name_email_similarity</th>\n",
              "      <th>prev_address_months_count</th>\n",
              "      <th>current_address_months_count</th>\n",
              "      <th>customer_age</th>\n",
              "      <th>days_since_request</th>\n",
              "      <th>intended_balcon_amount</th>\n",
              "      <th>payment_type</th>\n",
              "      <th>zip_count_4w</th>\n",
              "      <th>...</th>\n",
              "      <th>bank_months_count</th>\n",
              "      <th>has_other_cards</th>\n",
              "      <th>proposed_credit_limit</th>\n",
              "      <th>foreign_request</th>\n",
              "      <th>source</th>\n",
              "      <th>session_length_in_minutes</th>\n",
              "      <th>device_os</th>\n",
              "      <th>keep_alive_session</th>\n",
              "      <th>device_distinct_emails_8w</th>\n",
              "      <th>month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.218119</td>\n",
              "      <td>110</td>\n",
              "      <td>7</td>\n",
              "      <td>20</td>\n",
              "      <td>0.015684</td>\n",
              "      <td>-1.013463</td>\n",
              "      <td>AD</td>\n",
              "      <td>687</td>\n",
              "      <td>...</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>0</td>\n",
              "      <td>INTERNET</td>\n",
              "      <td>15.611467</td>\n",
              "      <td>windows</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.373086</td>\n",
              "      <td>29</td>\n",
              "      <td>7</td>\n",
              "      <td>20</td>\n",
              "      <td>0.024277</td>\n",
              "      <td>19.342285</td>\n",
              "      <td>AA</td>\n",
              "      <td>1230</td>\n",
              "      <td>...</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>0</td>\n",
              "      <td>INTERNET</td>\n",
              "      <td>5.911984</td>\n",
              "      <td>other</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.515218</td>\n",
              "      <td>-1</td>\n",
              "      <td>64</td>\n",
              "      <td>20</td>\n",
              "      <td>0.015865</td>\n",
              "      <td>-1.333985</td>\n",
              "      <td>AB</td>\n",
              "      <td>2045</td>\n",
              "      <td>...</td>\n",
              "      <td>25</td>\n",
              "      <td>1</td>\n",
              "      <td>200.0</td>\n",
              "      <td>0</td>\n",
              "      <td>INTERNET</td>\n",
              "      <td>4.732999</td>\n",
              "      <td>other</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.899345</td>\n",
              "      <td>-1</td>\n",
              "      <td>56</td>\n",
              "      <td>20</td>\n",
              "      <td>0.018821</td>\n",
              "      <td>-1.524424</td>\n",
              "      <td>AC</td>\n",
              "      <td>1260</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>0</td>\n",
              "      <td>INTERNET</td>\n",
              "      <td>8.210518</td>\n",
              "      <td>linux</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.794648</td>\n",
              "      <td>-1</td>\n",
              "      <td>64</td>\n",
              "      <td>50</td>\n",
              "      <td>0.027359</td>\n",
              "      <td>-1.254528</td>\n",
              "      <td>AB</td>\n",
              "      <td>1643</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>0</td>\n",
              "      <td>INTERNET</td>\n",
              "      <td>5.131610</td>\n",
              "      <td>other</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   fraud_bool  income  name_email_similarity  prev_address_months_count  \\\n",
              "0           0     0.1               0.218119                        110   \n",
              "1           0     0.1               0.373086                         29   \n",
              "2           0     0.2               0.515218                         -1   \n",
              "3           0     0.6               0.899345                         -1   \n",
              "4           0     0.9               0.794648                         -1   \n",
              "\n",
              "   current_address_months_count  customer_age  days_since_request  \\\n",
              "0                             7            20            0.015684   \n",
              "1                             7            20            0.024277   \n",
              "2                            64            20            0.015865   \n",
              "3                            56            20            0.018821   \n",
              "4                            64            50            0.027359   \n",
              "\n",
              "   intended_balcon_amount payment_type  zip_count_4w  ...  bank_months_count  \\\n",
              "0               -1.013463           AD           687  ...                 28   \n",
              "1               19.342285           AA          1230  ...                 22   \n",
              "2               -1.333985           AB          2045  ...                 25   \n",
              "3               -1.524424           AC          1260  ...                 -1   \n",
              "4               -1.254528           AB          1643  ...                  1   \n",
              "\n",
              "   has_other_cards  proposed_credit_limit  foreign_request    source  \\\n",
              "0                0                  500.0                0  INTERNET   \n",
              "1                0                  200.0                0  INTERNET   \n",
              "2                1                  200.0                0  INTERNET   \n",
              "3                0                  200.0                0  INTERNET   \n",
              "4                0                  200.0                0  INTERNET   \n",
              "\n",
              "  session_length_in_minutes  device_os  keep_alive_session  \\\n",
              "0                 15.611467    windows                   1   \n",
              "1                  5.911984      other                   0   \n",
              "2                  4.732999      other                   1   \n",
              "3                  8.210518      linux                   1   \n",
              "4                  5.131610      other                   0   \n",
              "\n",
              "  device_distinct_emails_8w  month  \n",
              "0                         1      7  \n",
              "1                         1      0  \n",
              "2                         1      4  \n",
              "3                         1      3  \n",
              "4                         1      0  \n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = df = pd.read_csv('BAF_200k.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbzVfHLMBBT0"
      },
      "source": [
        "2. Combien y a-t-il de catégories et quel est le nombre de données (nombre de transactions) dans ce dataset ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJDzNN8TBBT0",
        "outputId": "e65bb253-e528-4b8f-9cd4-63ad3eca8715"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nombre de colonnes 31\n",
            "nombre de données 200000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Code si besoin\n",
        "print(f\"nombre de colonnes {len(df.keys())}\")\n",
        "print(f\"nombre de données {len(df.index)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hKLPHNLBBT0"
      },
      "source": [
        "Nombre de catégories: 31\n",
        "\n",
        "Nombre de données: 200000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLzM8zdoBBT1"
      },
      "source": [
        "3. Afficher dans un graphe en baton (par exemple avec la librairie [matplotlib](https://matplotlib.org/) ou [seaborn](https://seaborn.pydata.org/)) le nombre de transactions étant des fraudes et celles n'en étant pas. Qu'est-ce que ce graphe vous permet d'observer ? Proposer une réponse courte."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kACoY1ZEBBT1"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-11T16:48:24.893821Z",
          "start_time": "2025-05-11T16:48:24.369872Z"
        },
        "id": "NhwicuG-bLwI"
      },
      "outputs": [],
      "source": [
        "fraud_counts = df['fraud_bool'].value_counts()\n",
        "\n",
        "fraud_counts.plot(kind='bar')\n",
        "plt.title(\"Répartition des fraudes\")\n",
        "plt.xlabel(\"fraud_bool\")\n",
        "plt.ylabel(\"Nombre d'occurrences\")\n",
        "plt.xticks(ticks=[0, 1], labels=['False', 'True'], rotation=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An1fCXydBBT1"
      },
      "source": [
        "Réponse ici:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bY9U_UHBBT1"
      },
      "source": [
        "4. Avant toute chose, nous allons séparer les données en trois ensembles : d'apprentissage (train_data), de validation (val_data) et de test (test_data). Nous ferons une **séparation stratifiée** car les données sont fortement débalancées entre fraudes ou non. Utiliser la fonction *train_test_split* de la librairie [scikit-learn train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) pour faire cette séparation (On séparera les données en ensemble de 70%, 15%, 15%)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_qQ9LamvBBT1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: (140000, 30), (140000,)\n",
            "Validation: (30000, 30), (30000,)\n",
            "Test: (30000, 30), (30000,)\n"
          ]
        }
      ],
      "source": [
        "X = df.drop(columns=[\"fraud_bool\"])\n",
        "y = df[\"fraud_bool\"]\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.30,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=0.50,\n",
        "    stratify=y_temp,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Train: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Validation: {X_val.shape}, {y_val.shape}\")\n",
        "print(f\"Test: {X_test.shape}, {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWPLKR77BBT1"
      },
      "source": [
        "5. S'assurer de l'intégrité des données en recherchant de potentielles valeurs manquantes. Afficher les catégories pour lesquelle il manque des données (celles-ci sont annotées par -1 dans le dataset), la quantitée de données manquantes par catégorie et le ratio de données manquantes par catégorie sur le total de données.\n",
        "\n",
        "**Important**: Avant chaque étape de processing de vos données, il est important de toujours se rappeler que les données de validation et de test sont considérées \"inconnues\" préalablement à leur utilisation, et doivent représenter la distribution réelle des données. Il faut donc bien réfléchir quoi faire avec ces données pour ne pas biaiser vos résultats, voir les rendre faux!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-12T00:21:15.010464Z",
          "start_time": "2025-05-12T00:21:14.930179Z"
        },
        "id": "kjsfuy_6BBT1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values for column fraud_bool:2\n",
            "Ratio of missing values:1e-05\n",
            "Missing values for column income:9\n",
            "Ratio of missing values:4.5e-05\n",
            "Missing values for column name_email_similarity:199947\n",
            "Ratio of missing values:0.999735\n",
            "Missing values for column prev_address_months_count:361\n",
            "Ratio of missing values:0.001805\n",
            "Missing values for column current_address_months_count:406\n",
            "Ratio of missing values:0.00203\n",
            "Missing values for column customer_age:9\n",
            "Ratio of missing values:4.5e-05\n",
            "Missing values for column days_since_request:199552\n",
            "Ratio of missing values:0.99776\n",
            "Missing values for column intended_balcon_amount:199768\n",
            "Ratio of missing values:0.99884\n",
            "Missing values for column payment_type:5\n",
            "Ratio of missing values:2.5e-05\n",
            "Missing values for column zip_count_4w:5855\n",
            "Ratio of missing values:0.029275\n",
            "Missing values for column velocity_6h:199939\n",
            "Ratio of missing values:0.999695\n",
            "Missing values for column velocity_24h:199973\n",
            "Ratio of missing values:0.999865\n",
            "Missing values for column velocity_4w:199934\n",
            "Ratio of missing values:0.99967\n",
            "Missing values for column bank_branch_count_8w:2238\n",
            "Ratio of missing values:0.01119\n",
            "Missing values for column date_of_birth_distinct_emails_4w:39\n",
            "Ratio of missing values:0.000195\n",
            "Missing values for column employment_status:7\n",
            "Ratio of missing values:3.5e-05\n",
            "Missing values for column credit_risk_score:524\n",
            "Ratio of missing values:0.00262\n",
            "Missing values for column email_is_free:2\n",
            "Ratio of missing values:1e-05\n",
            "Missing values for column housing_status:7\n",
            "Ratio of missing values:3.5e-05\n",
            "Missing values for column phone_home_valid:2\n",
            "Ratio of missing values:1e-05\n",
            "Missing values for column phone_mobile_valid:2\n",
            "Ratio of missing values:1e-05\n",
            "Missing values for column bank_months_count:33\n",
            "Ratio of missing values:0.000165\n",
            "Missing values for column has_other_cards:2\n",
            "Ratio of missing values:1e-05\n",
            "Missing values for column proposed_credit_limit:12\n",
            "Ratio of missing values:6e-05\n",
            "Missing values for column foreign_request:2\n",
            "Ratio of missing values:1e-05\n",
            "Missing values for column source:2\n",
            "Ratio of missing values:1e-05\n",
            "Missing values for column session_length_in_minutes:199464\n",
            "Ratio of missing values:0.99732\n",
            "Missing values for column device_os:5\n",
            "Ratio of missing values:2.5e-05\n",
            "Missing values for column keep_alive_session:2\n",
            "Ratio of missing values:1e-05\n",
            "Missing values for column device_distinct_emails_8w:4\n",
            "Ratio of missing values:2e-05\n",
            "Missing values for column month:8\n",
            "Ratio of missing values:4e-05\n"
          ]
        }
      ],
      "source": [
        "for column in df.columns:\n",
        "    emptyCount = 0\n",
        "    filtered_df = df[df[column] != -1]\n",
        "    median = 0\n",
        "    if(type(column[0]) == str):\n",
        "        median = df[column].mode()[0]\n",
        "    else:\n",
        "        median = filtered_df[column].median()\n",
        "        \n",
        "    missingData = df[column].value_counts(-1, 0).count()\n",
        "    ratio = missingData/len(df.index)\n",
        "    print(f\"Missing values for column {column}:{missingData}\")\n",
        "    print(f\"Ratio of missing values:{ratio}\")\n",
        "    \n",
        "    df[column].replace(-1, median)\n",
        "    \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVQYts2UBBT1"
      },
      "source": [
        "6. Remplacer les valeurs manquantes par la valeur médiane de la colonne correspondante. Bien réfléchir à ce que cela implique pour les données de validation et de test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-12T00:22:29.949235Z",
          "start_time": "2025-05-12T00:22:29.810051Z"
        },
        "id": "7sB3o01zBBT2"
      },
      "outputs": [],
      "source": [
        "# Deja fait dans le bloc precedent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0TQERxWBBT2"
      },
      "source": [
        "7. On notera que chacune des différentes catégories (colonnes) peut-être définie dans l'une des trois catégories suivantes:\n",
        "- Catégorie 1: Données numériques (ex. montant de la transaction, etc.)\n",
        "- Catégorie 2: Données catégorielles (ex. type de transaction, etc.)\n",
        "- Catégorie 3: Données de type booléen (ex. transaction frauduleuse ou non, etc.)\n",
        "\n",
        "Afficher une liste pour chaque catégorie de données (numérique *numeric_columns*, catégorielle *categorical_columns*, ou booléen *boolean_columns*), contenant le nom des colonnes (key) de données qui appartiennent à cette catégorie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-12T00:25:16.768238Z",
          "start_time": "2025-05-12T00:25:16.467157Z"
        },
        "id": "jO970dbpBBT2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alS9uGd1BBT2"
      },
      "source": [
        "8. Pour les données de chaque catégorie numérique, afficher la distribution de données superposée entre les transations frauduleuses et celles n'en étant pas (pour les données d'entraînement). Vous pouvez de nouveau utiliser la librairie [matplotlib](https://matplotlib.org/) ou [seaborn](https://seaborn.pydata.org/) pour afficher les distributions de données.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-11T16:49:14.041089Z",
          "start_time": "2025-05-11T16:48:49.186022Z"
        },
        "id": "QmtcSjUoBBT2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z0cd7KCBBT2"
      },
      "source": [
        "9. Considérant les distributions de données d'entrainement, donner une brève opinion / intuition sur la possibilité de classifier les transactions frauduleuses et celles n'en étant pas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9BnN5jYBBT2"
      },
      "source": [
        "Réponse ici:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qoMlLg2BBT2"
      },
      "source": [
        "# Partie 2 - Préparation des données et classification par template matching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKvzgwSsBBT3"
      },
      "source": [
        "La partie 1 aura permis de faire une première analyse des données, et un premier prétraitement des données en s'attaquant aux données manquantes. Nous allons maintenant approfondir le traitement des données pour préparer la classification, puis appliquer la méthode appelée \"template matching\".\n",
        "\n",
        "Un des points essentiels au bon fonctionnement des algorithmes d'apprentissage machine est, pour une grande majorité d'agorithmes, la normalization (scaling) des données. Cependant, le scaling des données peut-être fait de différentes manières, et peut aussi avoir un impact négatif sur les résultats de certains algorithmes si celle-ci n'est pas fait intelligemment. Vous êtes invités à survoler le papier suivant  - [paper link](https://www.sciencedirect.com/science/article/pii/S1568494622009735?fr=RR-2&ref=pdf_download&rr=93e5d0c1ecd57151) - pour motiver vos réponses aux questions à venir. N'hésitez pas à approfondir sa lecture au besoin.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzPQ1RiiCZPT"
      },
      "source": [
        "###  <font color=blue> Questions: </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEiyIfjqBBT3"
      },
      "source": [
        "1. Avant toute normalization de vos données, justifier pourquoi on ne pourra pas calculer la moyenne, l'écart-type, ou toute autre mesure statistique sur les bases val_data et test_data dans l'optique de leur normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_ZaFTxUBBT3"
      },
      "source": [
        "Réponse ici:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1eyVhxVBBT3"
      },
      "source": [
        "2. Commencer par choisir deux méthodes de normalisation différentes (ex. MinMaxScaler, StandardScaler, RobustScaler, etc.), en choisissant une normalization sensible aux outliers et une qui ne l'est pas. Appliquer chacune d'elles sur les données d'apprentissage. Normaliser aussi les données de validation et de test, tout en prenant en compte votre réponse à la question précédente.\n",
        "\n",
        "Remarque: On nommera les nouvelles bases de train validation et test de la façons suivante: {data}_{scaler}. ex: train_data_minmax, val_data_minmax, test_data_minmax, train_data_standard, val_data_standard, test_data_standard.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-12T00:39:03.403034Z",
          "start_time": "2025-05-12T00:39:03.092310Z"
        },
        "id": "mt_4ik9mBBT3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpSXBzy-BBT3"
      },
      "source": [
        "3. Nous allons maintenant classifier les opérations frauduleuses ou non en suivant une approche de *template matching* (plus proche prototype).\n",
        "\n",
        "3a. Commencer par définir une fonction qui retourne un template pour chaque classe (fraude ou non). Dans le cas présent, chaque template est défini comme étant la moyenne des valeurs par colonne.\n",
        "\n",
        "Plusieurs considérations sont importantes:\n",
        "\n",
        "-> On pourra prendre la moyenne des données numériques et binaires (boolean)\n",
        "\n",
        "-> Pour les données catégorielles, on pourra utiliser *pd.get_dummies* dans un premier temps (cette fonction créer des colonnes binaires pour chaque catégorie de la colonne - One-hot encoding). Ensuite, obtenir la moyenne de chaque colonne binaire.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0ZXQ6T6BBT3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6cSvayUBBT_"
      },
      "source": [
        "3b. En utilisant ces templates, proposer une fonction permettant de classifier les données d'un ensemble (validation/test) étant donné les templates produits issus de la fonction précédente. La fonction devra retourner un numpy array pour les données prédites et les données réelles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmh_f9t7BBT_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5WOPrnqBBUA"
      },
      "source": [
        "3c. Sur la base des fonctions précédentes, proposer une fonction d'évaluation qui appelle les fonction précédentes et retourne l'accuracy, une matrice de confusion, et un rapport de classification.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti-5MapIBBUA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihyN0abMBBUA"
      },
      "source": [
        "4. Appliquer votre fonction d'évaluation pour les données de validation et de test pour vos trois approches (pas de normalisation, et vos deux types de normalisation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_vVYdROBBUA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkrATwATBBUA"
      },
      "source": [
        "5. Que pensez-vous de la mesure d'accuracy, est-ce une bonne mesure de performance pour ce problème ? Pourquoi ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI8vakKgBBUA"
      },
      "source": [
        "Réponse ici:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ5yLmnyBBUA"
      },
      "source": [
        "6. D'après la matrice de confusion et le rapport de classification, que pensez-vous de l'approche de template matching ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INVB64UqBBUA"
      },
      "source": [
        "Réponse ici:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHi_CIIWDtAB"
      },
      "source": [
        "7. Pouvez-vous déjà tirer des conclusions sur l'importance de la normalisation via les résultats obtenus ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7rcKliDEzI6"
      },
      "source": [
        "Réponse ici:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ6bW1iTbLwQ"
      },
      "source": [
        "#### Résultats finaux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uYIg1tZbLwQ"
      },
      "source": [
        "Afficher les f1-score obtenus par classe dans les tableaux suivants pour le template matching avec ou sans normalisation (%). On utilisera les meilleurs résultats obtenus entre les deux normalisations.\n",
        "\n",
        "# Fraude\n",
        "| Ensemble | Sans normalisation | Avec normalisation (*nom de la normalisation ici*) |                                 \n",
        ":-|:-:|-:\n",
        "| Train    |  XX,XX%       |   XX,XX%      |                   \n",
        "| Val      |  XX,XX%       |   XX,XX%      |                             \n",
        "| Test     |  XX,XX%       |   XX,XX%      |       \n",
        "\n",
        "\n",
        "# Pas de fraude\n",
        "| Ensemble | Sans normalisation | Avec normalisation (*nom de la normalisation ici*) |                                 \n",
        ":-|:-:|-:\n",
        "| Train    |  XX,XX%       |   XX,XX%      |                   \n",
        "| Val      |  XX,XX%       |   XX,XX%      |                             \n",
        "| Test     |  XX,XX%       |   XX,XX%      |   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzYHFej-bLwQ"
      },
      "source": [
        "# Fin"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
